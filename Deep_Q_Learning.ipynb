{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scyc92/Deep-Q_Learning/blob/main/Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play Game com Agente Baseado em IA\n",
        "\n",
        "<img src=\"https://i.ibb.co/88qnZGK/catch-game.jpg\">\n",
        "\n",
        "Catch é um jogo de arcade muito simples, que você provavelmente já jogou. Os frutos estão caindo do topo da tela e o jogador precisa pegá-los com um cesto. Para cada fruta capturada, o jogador recebe um ponto. Para cada fruta perdida, o jogador perde um ponto. Nosso objetivo aqui é permitir que o computador jogue este game por si só.\n",
        "\n",
        "Ao jogar Catch, o jogador deve decidir entre 3 possíveis ações. O jogador pode mover a cesta para a esquerda, para a direita ou ficar na posição. A base para esta decisão é o estado atual do jogo, a posição do fruto e a posição do cesto, ambos visíveis na tela. Nosso objetivo é, portanto, criar um modelo que, dado o conteúdo da tela do jogo, escolha a ação que leva à maior pontuação possível.\n",
        "\n",
        "Esta tarefa poderia ser enquadrada como um problema de classificação simples. Poderíamos coletar dados de treinamento, permitindo que jogadores humanos experientes jogassem muitos jogos e, em seguida, treinassem um modelo para escolher a ação \"correta\" que espelha os jogadores experientes. Não é assim que os humanos aprendem no entanto. Os seres humanos podem aprender um jogo sem orientação. Isso é muito útil. Imagine que você teria que contratar um monte de especialistas para realizar uma tarefa milhares de vezes toda vez que você queria aprender algo tão simples como Catch. Seria muito caro e muito lento. Aqui, usaremos Deep Reinforcement Learning, onde o modelo aprende da experiência, em vez de dados de treinamento rotulados."
      ],
      "metadata": {
        "id": "JcBnVxAy4--1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXDVhYwT44Tl"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import seaborn\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "%matplotlib inline\n",
        "seaborn.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparando o Game\n",
        "\n",
        "No jogo, frutas, representadas por azulejos brancos, caem do topo. O objetivo é pegar os frutos com um basket (representado por azulejos brancos). Se você pegar uma fruta, você obtém um ponto (sua pontuação sobe por um), se você perder uma fruta, perdeu um (sua pontuação diminui).\n",
        "\n",
        "Não se preocupe muito com os detalhes da implementação, o foco aqui deve ser na IA, e não no jogo. Apenas certifique-se de executar esta célula para que ela seja definida."
      ],
      "metadata": {
        "id": "26DPs67p5lNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Catch(object):\n",
        "    def __init__(self, grid_size=10):\n",
        "        self.grid_size = grid_size\n",
        "        self.reset()\n",
        "\n",
        "    def _update_state(self, action):\n",
        "        \"\"\"\n",
        "        Input: ações e estados\n",
        "        Ouput: novos estados e recompensas\n",
        "        \"\"\"\n",
        "        state = self.state\n",
        "        if action == 0:\n",
        "          action = -1\n",
        "        elif action == 1:\n",
        "          action = 0\n",
        "        else:\n",
        "          action = 1\n",
        "        f0, f1, basket = state[0]\n",
        "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
        "        f0 += 1\n",
        "        out = np.asarray([f0, f1, new_basket])\n",
        "        out = out[np.newaxis]\n",
        "        assert len(out.shape) == 2\n",
        "        self.state = out        \n",
        "        \n",
        "    def _draw_state(self):\n",
        "        im_size = (self.grid_size,)*2\n",
        "        state = self.state[0]\n",
        "        canvas = np.zeros(im_size)\n",
        "        canvas[state[0], state[1]] = 1  # desenha fruta\n",
        "        canvas[-1, int(state[2])-1:int(state[2]) + 2] = 1  # desenha basket\n",
        "\n",
        "        return canvas\n",
        "        \n",
        "    def _get_reward(self):\n",
        "      fruit_row, fruit_col, basket = self.state[0]\n",
        "      if fruit_row == self.grid_size-1:\n",
        "        if abs(fruit_col - basket) <= 1:\n",
        "          return 1\n",
        "        else:\n",
        "          return -1\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "    def _is_over(self):\n",
        "      if self.state[0, 0] == self.grid_size-1:\n",
        "        return True\n",
        "      else:\n",
        "        return False        \n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self._draw_state()\n",
        "        return canvas.reshape((1, -1))\n",
        "\n",
        "    def act(self, action):\n",
        "        self._update_state(action)\n",
        "        reward = self._get_reward()\n",
        "        game_over = self._is_over()\n",
        "        return self.observe(), reward, game_over\n",
        "\n",
        "    def reset(self):\n",
        "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
        "        m = np.random.randint(0, self.grid_size-2, size=1)\n",
        "        self.state = np.asarray([0, n, m])[np.newaxis]"
      ],
      "metadata": {
        "id": "-fscFjOW5jpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Além de definir o jogo, precisamos definir algumas variáveis e funções auxiliares. Execute as células abaixo para defini-las."
      ],
      "metadata": {
        "id": "nteFal6q5sgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O último time frame faz o controle do quadro em que estamos\n",
        "last_frame_time = 0\n",
        "\n",
        "# Traduz as ações para palavras humanas legíveis\n",
        "translate_action = [\"Left\",\"Stay\",\"Right\",\"Create Ball\",\"End Test\"]\n",
        "\n",
        "# Tamanho do campo de jogo\n",
        "grid_size = 10"
      ],
      "metadata": {
        "id": "08DjHcdX5qfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_screen(action,points,input_t):\n",
        "    # Renderiza a tela do jogo\n",
        "    global last_frame_time\n",
        "    print(\"Action %s, Points: %d\" % (translate_action[action],points))\n",
        "    \n",
        "    # Somente mostra a tela do jogo se não for game over\n",
        "    if(\"End\" not in translate_action[action]):\n",
        "        plt.imshow(input_t.reshape((grid_size,)*2), interpolation='none', cmap='gray')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "    last_frame_time = set_max_fps(last_frame_time)\n",
        "    \n",
        "    \n",
        "def set_max_fps(last_frame_time,FPS = 1):\n",
        "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
        "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
        "    if sleep_time > 0:\n",
        "        time.sleep(sleep_time)\n",
        "    return current_milli_time()"
      ],
      "metadata": {
        "id": "jWwTQ-GN5uFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Reinforcement Learning\n",
        "\n",
        "Agora vamos a parte emocionante.\n",
        "\n",
        "\n",
        "## Q-Learning \n",
        "\n",
        "No Q-learning, definimos uma função Q(s, a) que representa a recompensa futura máxima com desconto quando executamos a ação a no estado s e continuamos de forma óptima a partir desse ponto.\n",
        "\n",
        "Uma boa maneira de entender Q-learning é comparar jogar Catch com jogar xadrez. Em ambos os jogos, você recebe um estado s (xadrez: posições das figuras no tabuleiro, Catch: localização do fruto e do cesto), no qual você deve tomar uma ação a (xadrez: mova uma figura, Catch: move o cesto à esquerda, à direita ou fica onde você está). Como resultado, haverá alguma recompensa e um novo estado s '. O problema tanto com Catch quanto com o xadrez é que as recompensas não aparecerão imediatamente após você ter tomado a ação. Em Catch, você só ganha recompensas quando os frutos caem na cesta ou caem no chão, e no xadrez você só ganha uma recompensa quando ganha ou perde o jogo. As recompensas são _sparsely distributed_, na maioria das vezes, r será 0. Quando há uma recompensa, nem sempre é resultado da ação tomada imediatamente antes. Algumas ações tomadas muito antes podem ter causado a vitória. Determinar qual ação é responsável pela recompensa é muitas vezes referido como _credit assignment problem_.\n",
        "\n",
        "Como as recompensas são atrasadas, os bons jogadores de xadrez não escolhem suas peças apenas pela recompensa imediata, mas pela recompensa futura esperada. Eles não só pensam sobre se eles podem eliminar uma figura de oponentes no próximo movimento, mas como tomar uma determinada ação agora irá ajudá-los a longo prazo.\n",
        "\n",
        "No Q-learning, escolhemos nossa ação com base na maior recompensa futura esperada. Enquanto no estado s, estimamos a recompensa futura para cada ação possível a. Assumimos que depois de terem feito uma ação e mudado para o próximo estado s', tudo funciona perfeitamente. Como em finanças, nós descontamos recompensas futuras, já que elas são incertas.\n",
        "\n",
        "A recompensa futura esperada Q(s, a) dado um estado s e uma ação a é, portanto, a recompensa r que segue diretamente de um mais a recompensa futura esperada Q(s', a') se a ação ideal a' for tomada em o seguinte estado s', descontado pelo fator de desconto gama.\n",
        "\n",
        "Q(s,a) = r + gamma * max Q(s’,a’)\n",
        "\n",
        "Os bons jogadores de xadrez são muito bons na estimativa de recompensas futuras em sua cabeça. Em outras palavras, sua função Q (s, a) é muito precisa. A maioria das práticas de xadrez gira em torno do desenvolvimento de uma melhor função Q. Os jogadores examinam muitos jogos antigos para saber como os movimentos específicos se desenrolaram no passado e a probabilidade de uma determinada ação levar a vitória.\n",
        "\n",
        "Mas como podemos estimar uma boa função Q? É aqui que as redes neurais entram em jogo.\n",
        "\n",
        "\n",
        "## Como Treinar o Agente\n",
        "\n",
        "Ao jogar, geramos muitas experiências consistindo no estado inicial s, na ação tomada a, na recompensa ganha r e no estado que seguiu s'. Essas experiências são nossos dados de treinamento. Podemos enquadrar o problema de estimar Q(s, a) como um simples problema de regressão. Dado um vetor de entrada consistindo de s e uma rede neural é suposto prever o valor de Q(s, a) igual ao alvo: r + gama * max Q(s', a'). Se for bom prever Q(s, a) para diferentes estados s e ações a, temos uma boa aproximação de Q. Observe que Q(s', a') é também uma predição da rede neural que estamos treinando.\n",
        "\n",
        "Dado um lote de experiências < s, a, r, s’ >, o processo de treinamento então é o seguinte:\n",
        "\n",
        "1. Para cada ação possível a' (esquerda, direita, permanência), preveja a recompensa futura esperada Q(s', a') usando a rede neural\n",
        "2. Escolha o valor mais alto das três predições max Q(s', a')\n",
        "3. Calcule r + gama * max Q (s', a'). Este é o valor alvo da rede neural.\n",
        "4. Treina a rede neural usando a função de perda 1/2(predicted_Q(s,a) - target)^2\n",
        "\n",
        "Durante a jogabilidade, todas as experiências são armazenadas em uma memória de repetição. Esta é a classe abaixo.\n",
        "\n",
        "A função de lembrança simplesmente salva uma experiência em uma lista. A função get_batch executa os passos 1 a 3 da lista acima e retorna uma entrada e um vetor de destino. O treinamento real é feito em uma função discutida abaixo.\n"
      ],
      "metadata": {
        "id": "OzNypIOi5xqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplay(object):\n",
        "    def __init__(self, max_memory=100, discount=.9):\n",
        "        \"\"\"\n",
        "        Setup\n",
        "        max_memory: o número máximo de experiências que queremos armazenar\n",
        "        memory: uma lista de experiências\n",
        "        discount: o fator de desconto para a experiência futura\n",
        "        \n",
        "        Na memória, a informação se o jogo terminou no estado é armazenada separadamente em uma matriz aninhada\n",
        "        [...\n",
        "        [experience, game_over]\n",
        "        [experience, game_over]\n",
        "        ...]\n",
        "        \"\"\"\n",
        "        self.max_memory = max_memory\n",
        "        self.memory = list()\n",
        "        self.discount = discount\n",
        "\n",
        "    def remember(self, states, game_over):\n",
        "        self.memory.append([states, game_over])\n",
        "  \n",
        "        if len(self.memory) > self.max_memory:\n",
        "          del self.memory[0]\n",
        "\n",
        "    def get_batch(self, model, batch_size=10):\n",
        "        \n",
        "        len_memory = len(self.memory)\n",
        "        new_actions = model.output_shape[-1]\n",
        "\n",
        "        env_dim = self.memory[0][0][0].shape[1]\n",
        "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
        "        targets = np.zeros((inputs.shape[0], num_actions))\n",
        "        \n",
        "        # Nós desenhamos estados para aprender aleatoriamente\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
        "\n",
        "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
        "            \n",
        "            # Também precisamos saber se o jogo terminou nesse estado\n",
        "            game_over = self.memory[idx][1]\n",
        "\n",
        "            # Adicione o estado s à entrada\n",
        "            inputs[i:i+1] = state_t\n",
        "            \n",
        "            # Primeiro, preenchemos os valores-alvo com as previsões do modelo. \n",
        "            # Eles não serão afetados pelo treinamento (uma vez que a perda de treinamento para eles é 0)\n",
        "            targets[i] = model.predict(state_t)[0]\n",
        "            \n",
        "            \"\"\"\n",
        "            Se o jogo acabou, a recompensa esperada Q (s, a) deve ser a recompensa final r.\n",
        "            Ou então o target value é r + gamma * max Q(s’,a’)\n",
        "            \"\"\"\n",
        "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
        "            \n",
        "            # Se o jogo acabou, a recompensa é a recompensa final.\n",
        "            if game_over:  \n",
        "                targets[i, action_t] = reward_t\n",
        "            else:\n",
        "                # r + gamma * max Q(s’,a’)\n",
        "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
        "        return inputs, targets\n"
      ],
      "metadata": {
        "id": "fFg5d4HZ5vmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo o Modelo\n",
        "\n",
        "Agora é hora de definir o modelo que irá aprender Q. Estamos usando o Keras como frontend para Tensorflow ou Theano. Nosso modelo de linha de base é uma rede muito simples de 3 camadas densas. Você pode brincar com modelos mais complexos e ver se você pode melhorar o desempenho."
      ],
      "metadata": {
        "id": "vYO8F7PI57Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model(grid_size,num_actions,hidden_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
        "    model.add(Dense(hidden_size, activation='relu'))\n",
        "    model.add(Dense(num_actions))\n",
        "    model.compile(SGD(lr = .1), \"mse\")\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "eSlq5kRL53oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parâmetros\n",
        "\n",
        "Antes de começar a treinar, precisamos definir alguns parâmetros. Você também pode experimentar com esses."
      ],
      "metadata": {
        "id": "RPj_cl795-7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros\n",
        "epsilon = .1      # Exploração\n",
        "num_actions = 3   # [move_left, stay, move_right]\n",
        "max_memory = 500  # Número máximo de experiências que estamos armazenando\n",
        "hidden_size = 100 # Tamanho das camadas ocultas\n",
        "batch_size = 1    # Número de experiências que usamos para treinar por lote\n",
        "grid_size = 10    # Tamanho do campo de jogo"
      ],
      "metadata": {
        "id": "yi9u9DXp59VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo\n",
        "model = baseline_model(grid_size,num_actions,hidden_size)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4OVDmmBL6BVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define environment/game\n",
        "env = Catch(grid_size)\n",
        "\n",
        "# Inicializa o objeto de repetição da experiência\n",
        "exp_replay = ExperienceReplay(max_memory=max_memory)"
      ],
      "metadata": {
        "id": "SpMK_Yr86Caa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o Modelo\n",
        "\n",
        "O treinamento é relativamente direto. Nós deixamos o modelo jogar o jogo. Enquanto joga, ele gera dados de treinamento na forma de experiências. Usamos esses dados de treinamento para treinar nosso estimador Q."
      ],
      "metadata": {
        "id": "WTsc1QNU6Fgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs, verbose = 1):\n",
        "    win_cnt = 0\n",
        "    win_hist = []\n",
        "    for e in range(epochs):\n",
        "      loss = 0\n",
        "      env.reset()\n",
        "      game_over=False\n",
        "      input_t = env.observe()\n",
        "      while not game_over:\n",
        "        input_tm1 = input_t\n",
        "        if np.random.rand() <= epsilon:\n",
        "          action = np.random.randint(0, num_actions, size=1)\n",
        "        else:\n",
        "          q = model.predict(input_tm1)\n",
        "          action = np.argmax(q[0])\n",
        "        \n",
        "        input_t, reward, game_over = env.act(action)\n",
        "\n",
        "        if reward == 1:\n",
        "          win_cnt += 1\n",
        "        \n",
        "        exp_replay.remember([input_tm1, action, reward, input_t], game_over)\n",
        "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
        "        batch_loss = model.train_on_batch(inputs, targets)\n",
        "\n",
        "        loss += batch_loss\n",
        "\n",
        "        if verbose > 0:\n",
        "          print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win Count {}\".format(e, epochs, loss, win_cnt))       \n",
        "        win_hist.append(win_cnt)\n",
        "    return win_hist"
      ],
      "metadata": {
        "id": "E7KfRyBd6EDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playing Vários Games\n",
        "\n",
        "Para se tornar um bom jogador, nosso modelo precisa jogar _many_ games. Descobri que, após cerca de 4.000 jogos, tornou-se um jogador decente. Por causa de um notebook legível, desabilitamos a saída do treinador aqui. Veja a seção sobre avaliação do progresso abaixo para um gráfico."
      ],
      "metadata": {
        "id": "8dJcjmV36MNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de jogos jogados no treinamento. O modelo precisa de cerca de 4.000 jogos até que ele jogue bem\n",
        "epoch = 5 \n",
        "\n",
        "# Treinando o Modelo\n",
        "hist = train(model, epoch, verbose=1)\n",
        "print(\"Treinamento Concluído!\")"
      ],
      "metadata": {
        "id": "PBahxU-c6LHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando o Modelo\n",
        "\n",
        "Agora que temos um ótimo jogador Catch à mão, queremos vê-lo em ação! A função de teste é muito semelhante à função do trem. Só que nos testes não salvamos as experiências e treinamos nelas. Mas agora podemos usar as funções de renderização definidas acima para assistir nosso modelo de jogo!"
      ],
      "metadata": {
        "id": "Xu9d3ue36RHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model):\n",
        "    global last_frame_time\n",
        "    plt.ion()\n",
        "    env = Catch(grid_size)\n",
        "    c = 0\n",
        "    last_frame_time = 0\n",
        "    points = 0\n",
        "    \n",
        "    for e in range(10):\n",
        "        loss = 0.\n",
        "        env.reset()\n",
        "        game_over = False\n",
        "        input_t = env.observe()\n",
        "        c += 1\n",
        "        while not game_over:\n",
        "            input_tm1 = input_t\n",
        "            q = model.predict(input_tm1)\n",
        "            action = np.argmax(q[0])\n",
        "            input_t, reward, game_over = env.act(action)\n",
        "            points += reward\n",
        "            display_screen(action,points,input_t)\n",
        "            c += 1\n",
        "\n",
        "test(model)"
      ],
      "metadata": {
        "id": "wRgij1ha6O17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliando o Progresso\n",
        "\n",
        "Essa demo é bastante impressionante, hein? Antes de terminar esta pequena excursão, vamos ter um olhar mais atento sobre o modo como nosso modelo realmente aprendeu. Mais cedo, salvamos a história das vitórias. Agora, podemos traçar a média móvel da diferença, ou seja, quantas vitórias adicionais marcaram o modelo por jogo extra. 1 vitória extra por jogo significa que o modelo ganha cada jogo (pega todas as frutas), 0 significa que ela perde todas elas. Como você pode ver, o modelo se aproxima de uma taxa de vitória de 100% ao longo do tempo. Após 4000 peças, o modelo ganha de forma relativamente consistente. As quedas aleatórias no desempenho são provavelmente devido ao epsilon de escolha aleatória que são feitas de tempos em tempos. Um modelo melhor se aproximaria de 100% mais rápido."
      ],
      "metadata": {
        "id": "Z56wDHHQ6Zn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average_diff(a, n=100):\n",
        "    diff = np.diff(a)\n",
        "    ret = np.cumsum(diff, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "plt.plot(moving_average_diff(hist))\n",
        "plt.ylabel('Média de Vitórias Por Game')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lq7-OSex6WxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aIfVsQWGzNQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Referências:\n",
        "\n",
        "Data Science Academy - Formação IA\n",
        "\n",
        "Desmystifying Deep Reinforcement Learning\n",
        "https://www.intelnervana.com/demystifying-deep-reinforcement-learning/\n",
        "\n",
        "Deep Reinforcement Learning Stanford\n",
        "http://rll.berkeley.edu/deeprlcourse/\n",
        "\n",
        "Deep Mind\n",
        "https://deepmind.com/blog/deep-reinforcement-learning/\n"
      ],
      "metadata": {
        "id": "gVuABcB16d0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-_qGqnZw6i30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}